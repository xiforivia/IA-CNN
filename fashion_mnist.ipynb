{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processamento():\n",
    "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "    train_images = train_images.reshape((60000,28,28,1))\n",
    "    train_images = train_images.astype('float32')/255\n",
    "\n",
    "    test_images = test_images.reshape((10000,28,28,1))\n",
    "    test_images = test_images.astype('float32')/255\n",
    "\n",
    "    train_labels = to_categorical(train_labels)\n",
    "    test_labels = to_categorical(test_labels)\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a arquitetura da CNN\n",
    "def criar_modelo(conv_layers, filters, dense_size):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Adicionar camadas de convolução-pooling\n",
    "    for i in range(conv_layers):\n",
    "        model.add(layers.Conv2D(filters=filters, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu', input_shape = (28,28,1)))\n",
    "        print(\"Conv2D\")\n",
    "        if i != conv_layers - 1 or conv_layers == 1: #se não for a última camada de conv, faz MaxPooling2D, com exceção se tiver somente 1 camada de conv, nesse caso faz MaxPooling2D (mesmo ela sendo a última também)\n",
    "            print(\"MaxPooling2D\")\n",
    "            model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(dense_size, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax')) # 10 é o número de classes\n",
    "    \n",
    "    model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels, test_images, test_labels = pre_processamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir os valores fixos dos parâmetros\n",
    "filters = 32\n",
    "dense_size = 64\n",
    "\n",
    "# Testar diferentes quantidades de camadas de convolução-pooling\n",
    "conv_layers_list = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D\n",
      "MaxPooling2D\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 25s 25ms/step - loss: 0.4640 - accuracy: 0.8361\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 25s 27ms/step - loss: 0.2935 - accuracy: 0.8957\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 21s 22ms/step - loss: 0.2522 - accuracy: 0.9081\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 23s 25ms/step - loss: 0.2237 - accuracy: 0.9179\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 25s 27ms/step - loss: 0.2014 - accuracy: 0.9257\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2802 - accuracy: 0.9057\n",
      "test_loss: 0.28022149205207825 \n",
      "test_acc: 0.9057000279426575\n",
      "Essa acurácia significa que o modelo usando layer: 1, filter: 32 e tamanho da camada densa: 64 é  capaz de classificar corretamente 90.6%  das imagens\n",
      "Conv2D\n",
      "MaxPooling2D\n",
      "Conv2D\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 41s 43ms/step - loss: 0.4398 - accuracy: 0.8428\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 36s 38ms/step - loss: 0.2768 - accuracy: 0.8992\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 36s 38ms/step - loss: 0.2314 - accuracy: 0.9165\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 36s 39ms/step - loss: 0.2006 - accuracy: 0.9290\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 36s 39ms/step - loss: 0.1768 - accuracy: 0.9351\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2550 - accuracy: 0.9080\n",
      "test_loss: 0.2550165057182312 \n",
      "test_acc: 0.9079999923706055\n",
      "Essa acurácia significa que o modelo usando layer: 2, filter: 32 e tamanho da camada densa: 64 é  capaz de classificar corretamente 90.8%  das imagens\n",
      "Conv2D\n",
      "MaxPooling2D\n",
      "Conv2D\n",
      "MaxPooling2D\n",
      "Conv2D\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 46s 48ms/step - loss: 0.4815 - accuracy: 0.8223\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 42s 45ms/step - loss: 0.3001 - accuracy: 0.8911\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 42s 45ms/step - loss: 0.2555 - accuracy: 0.9061\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 40s 43ms/step - loss: 0.2260 - accuracy: 0.9168\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 37s 39ms/step - loss: 0.2066 - accuracy: 0.9237\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.2627 - accuracy: 0.9008\n",
      "test_loss: 0.26271530985832214 \n",
      "test_acc: 0.9007999897003174\n",
      "Essa acurácia significa que o modelo usando layer: 3, filter: 32 e tamanho da camada densa: 64 é  capaz de classificar corretamente 90.1%  das imagens\n",
      "Portanto, a melhor layer é a 2, que possui 90.8 de acurácia.\n"
     ]
    }
   ],
   "source": [
    "melhor_acc = 0\n",
    "melhor_layer = 0\n",
    "for layer in conv_layers_list:\n",
    "    model = criar_modelo(layer, filters, dense_size)\n",
    "    model.fit(train_images, train_labels, epochs=5, batch_size = 64)\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "    print(\"test_loss:\", test_loss, \"\\ntest_acc:\", test_acc)\n",
    "    print(f\"Essa acurácia significa que o modelo usando layer: {layer}, filter: {filters} e tamanho da camada densa: {dense_size} é  capaz de classificar corretamente {round(test_acc*100, 1)}%  das imagens\")\n",
    "    if test_acc > melhor_acc:\n",
    "        melhor_acc = test_acc\n",
    "        melhor_layer = layer\n",
    "print(f\"Portanto, a melhor layer é a {melhor_layer}, que possui {round(melhor_acc*100, 1)} de acurácia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_list = [16, 32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D\n",
      "MaxPooling2D\n",
      "Conv2D\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 21ms/step - loss: 0.4651 - accuracy: 0.8326\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.3093 - accuracy: 0.8877\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.2594 - accuracy: 0.9043\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.2282 - accuracy: 0.9163\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.2047 - accuracy: 0.9246\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.2746 - accuracy: 0.9010\n",
      "test_loss: 0.2745843231678009 \n",
      "test_acc: 0.9010000228881836\n",
      "Essa acurácia significa que o modelo usando layer: 2, filter: 16 e tamanho da camada densa: 64 é capaz de classificar corretamente 90.1%  das imagens\n",
      "Conv2D\n",
      "MaxPooling2D\n",
      "Conv2D\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 35s 36ms/step - loss: 0.4488 - accuracy: 0.8404\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 34s 36ms/step - loss: 0.2853 - accuracy: 0.8967\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 34s 36ms/step - loss: 0.2369 - accuracy: 0.9133\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 34s 36ms/step - loss: 0.2050 - accuracy: 0.9251\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 34s 37ms/step - loss: 0.1800 - accuracy: 0.9340\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2441 - accuracy: 0.9145\n",
      "test_loss: 0.2441481202840805 \n",
      "test_acc: 0.9144999980926514\n",
      "Essa acurácia significa que o modelo usando layer: 2, filter: 32 e tamanho da camada densa: 64 é capaz de classificar corretamente 91.4%  das imagens\n",
      "Conv2D\n",
      "MaxPooling2D\n",
      "Conv2D\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 74s 78ms/step - loss: 0.4263 - accuracy: 0.8476\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.2608 - accuracy: 0.9050\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 73s 77ms/step - loss: 0.2138 - accuracy: 0.9235\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.1818 - accuracy: 0.9349\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.1578 - accuracy: 0.9414\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.2482 - accuracy: 0.9168\n",
      "test_loss: 0.24820488691329956 \n",
      "test_acc: 0.9168000221252441\n",
      "Essa acurácia significa que o modelo usando layer: 2, filter: 64 e tamanho da camada densa: 64 é capaz de classificar corretamente 91.7%  das imagens\n",
      "Portanto, o melhor filter é o 64, que possui 91.7 de acurácia.\n"
     ]
    }
   ],
   "source": [
    "melhor_acc = 0\n",
    "melhor_filter = 0\n",
    "for filters in filters_list:\n",
    "    model = criar_modelo(melhor_layer, filters, dense_size)\n",
    "    model.fit(train_images, train_labels, epochs=5, batch_size = 64)\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "    print(\"test_loss:\", test_loss, \"\\ntest_acc:\", test_acc)\n",
    "    print(f\"Essa acurácia significa que o modelo usando layer: {melhor_layer}, filter: {filters} e tamanho da camada densa: {dense_size} é capaz de classificar corretamente {round(test_acc*100, 1)}%  das imagens\")\n",
    "    if test_acc > melhor_acc:\n",
    "        melhor_acc = test_acc\n",
    "        melhor_filter = filters\n",
    "print(f\"Portanto, o melhor filter é o {melhor_filter}, que possui {round(melhor_acc*100, 1)} de acurácia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_size_list = [64, 128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D\n",
      "MaxPooling2D\n",
      "Conv2D\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 71s 74ms/step - loss: 0.4138 - accuracy: 0.8520\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 69s 74ms/step - loss: 0.2555 - accuracy: 0.9075\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 72s 76ms/step - loss: 0.2086 - accuracy: 0.9242\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.1767 - accuracy: 0.9347\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 73s 77ms/step - loss: 0.1523 - accuracy: 0.9441\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.2348 - accuracy: 0.9169\n",
      "test_loss: 0.2348276823759079 \n",
      "test_acc: 0.9168999791145325\n",
      "Essa acurácia significa que o modelo usando layer: 2, filter: 64 e tamanho da camada densa: 64 é capaz de classificar corretamente 91.7%  das imagens\n",
      "Conv2D\n",
      "MaxPooling2D\n",
      "Conv2D\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 80s 85ms/step - loss: 0.4004 - accuracy: 0.8547\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.2456 - accuracy: 0.9092\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 66s 71ms/step - loss: 0.1985 - accuracy: 0.9266\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.1649 - accuracy: 0.9397\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 67s 72ms/step - loss: 0.1379 - accuracy: 0.9495\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.2297 - accuracy: 0.9218\n",
      "test_loss: 0.22974519431591034 \n",
      "test_acc: 0.9218000173568726\n",
      "Essa acurácia significa que o modelo usando layer: 2, filter: 64 e tamanho da camada densa: 128 é capaz de classificar corretamente 92.2%  das imagens\n",
      "Conv2D\n",
      "MaxPooling2D\n",
      "Conv2D\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 81s 86ms/step - loss: 0.3926 - accuracy: 0.8588\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 79s 85ms/step - loss: 0.2394 - accuracy: 0.9121\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 79s 84ms/step - loss: 0.1905 - accuracy: 0.9301\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 79s 85ms/step - loss: 0.1549 - accuracy: 0.9428\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 80s 85ms/step - loss: 0.1247 - accuracy: 0.9535\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.2400 - accuracy: 0.9222\n",
      "test_loss: 0.23997081816196442 \n",
      "test_acc: 0.9222000241279602\n",
      "Essa acurácia significa que o modelo usando layer: 2, filter: 64 e tamanho da camada densa: 256 é capaz de classificar corretamente 92.2%  das imagens\n",
      "Portanto, o melhor tamanho da camada densa é 256, que possui 92.2 de acurácia.\n"
     ]
    }
   ],
   "source": [
    "melhor_acc = 0\n",
    "melhor_dense = 0\n",
    "for dense_size in dense_size_list:\n",
    "    model = criar_modelo(melhor_layer, melhor_filter, dense_size)\n",
    "    model.fit(train_images, train_labels, epochs=5, batch_size = 64)\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "    print(\"test_loss:\", test_loss, \"\\ntest_acc:\", test_acc)\n",
    "    print(f\"Essa acurácia significa que o modelo usando layer: {melhor_layer}, filter: {melhor_filter} e tamanho da camada densa: {dense_size} é capaz de classificar corretamente {round(test_acc*100, 1)}%  das imagens\")\n",
    "    if test_acc > melhor_acc:\n",
    "        melhor_acc = test_acc\n",
    "        melhor_dense = dense_size\n",
    "print(f\"Portanto, o melhor tamanho da camada densa é {melhor_dense}, que possui {round(melhor_acc*100, 1)} de acurácia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ao final, a melhor combinação foi: \n",
      "layer: 2, filter: 64 e tamanho da camada densa: 256\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ao final, a melhor combinação foi: \\nlayer: {melhor_layer}, filter: {melhor_filter} e tamanho da camada densa: {melhor_dense}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.9083999991416931 same\n",
    "\n",
    "# 90.7 valid\n",
    "\n",
    "# 91.1 valid input\n",
    "\n",
    "# 91.7 same input\n",
    "\n",
    "# 91.2 same "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que é dropout? Avalie diferentes porcentagens de dropout. Quanto dropout é melhor?\n",
    "\n",
    "O Dropout é uma técnica de regularização utilizada para reduzir o overfitting em redes neurais. Durante o treinamento, uma proporção dos neurônios é aleatoriamente \"desligada\" (dropout) em cada atualização do gradiente, o que força a rede a aprender recursos mais robustos e evita a dependência excessiva de neurônios específicos.\n",
    "\n",
    "Vamos modificar a função create_cnn_model_with_dense_size para adicionar uma camada Dropout antes da camada densa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a arquitetura da CNN\n",
    "def criar_modelo_com_dropout(conv_layers, filters, dense_size, dropout_rate):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Adicionar camadas de convolução-pooling\n",
    "    for i in range(conv_layers):\n",
    "        model.add(layers.Conv2D(filters=filters, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu', input_shape = (28,28,1)))\n",
    "        print(\"Conv2D\")\n",
    "        if i != conv_layers - 1 or conv_layers == 1: #se não for a última camada de conv, faz MaxPooling2D, com exceção se tiver somente 1 camada de conv, nesse caso faz MaxPooling2D (mesmo ela sendo a última também)\n",
    "            print(\"MaxPooling2D\")\n",
    "            model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(dense_size, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(10, activation='softmax')) # 10 é o número de classes\n",
    "    \n",
    "    model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D\n",
      "MaxPooling2D\n",
      "Conv2D\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 80s 85ms/step - loss: 0.4003 - accuracy: 0.8554\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 79s 85ms/step - loss: 0.2439 - accuracy: 0.9103\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 80s 85ms/step - loss: 0.1977 - accuracy: 0.9269\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 80s 85ms/step - loss: 0.1643 - accuracy: 0.9391\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 79s 85ms/step - loss: 0.1360 - accuracy: 0.9513\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.2513 - accuracy: 0.9201\n",
      "test_loss: 0.2513017952442169 \n",
      "test_acc: 0.9200999736785889\n",
      "Essa acurácia significa que o modelo usando layer: 2, filter: 64, tamanho da camada densa: 256 e dropout: 0.1 é capaz de classificar corretamente 92.0%  das imagens\n",
      "Conv2D\n",
      "MaxPooling2D\n",
      "Conv2D\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 80s 85ms/step - loss: 0.4155 - accuracy: 0.8513\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 79s 85ms/step - loss: 0.2578 - accuracy: 0.9054\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 80s 85ms/step - loss: 0.2121 - accuracy: 0.9225\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 80s 85ms/step - loss: 0.1819 - accuracy: 0.9337\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 80s 85ms/step - loss: 0.1600 - accuracy: 0.9417\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.2365 - accuracy: 0.9157\n",
      "test_loss: 0.23653700947761536 \n",
      "test_acc: 0.9157000184059143\n",
      "Essa acurácia significa que o modelo usando layer: 2, filter: 64, tamanho da camada densa: 256 e dropout: 0.3 é capaz de classificar corretamente 91.6%  das imagens\n",
      "Conv2D\n",
      "MaxPooling2D\n",
      "Conv2D\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 80s 85ms/step - loss: 0.4432 - accuracy: 0.8425\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 79s 85ms/step - loss: 0.2811 - accuracy: 0.8986\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 79s 85ms/step - loss: 0.2391 - accuracy: 0.9140\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 95s 101ms/step - loss: 0.2126 - accuracy: 0.9236\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 104s 110ms/step - loss: 0.1930 - accuracy: 0.9309\n",
      "313/313 [==============================] - 6s 17ms/step - loss: 0.2379 - accuracy: 0.9133\n",
      "test_loss: 0.23789025843143463 \n",
      "test_acc: 0.9132999777793884\n",
      "Essa acurácia significa que o modelo usando layer: 2, filter: 64, tamanho da camada densa: 256 e dropout: 0.5 é capaz de classificar corretamente 91.3%  das imagens\n",
      "Conv2D\n",
      "MaxPooling2D\n",
      "Conv2D\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 107s 113ms/step - loss: 0.4960 - accuracy: 0.8245\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 105s 112ms/step - loss: 0.3129 - accuracy: 0.8909\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 105s 112ms/step - loss: 0.2709 - accuracy: 0.9058\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 104s 111ms/step - loss: 0.2513 - accuracy: 0.9118\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 105s 112ms/step - loss: 0.2394 - accuracy: 0.9164\n",
      "313/313 [==============================] - 6s 17ms/step - loss: 0.2408 - accuracy: 0.9172\n",
      "test_loss: 0.24083290994167328 \n",
      "test_acc: 0.9172000288963318\n",
      "Essa acurácia significa que o modelo usando layer: 2, filter: 64, tamanho da camada densa: 256 e dropout: 0.7 é capaz de classificar corretamente 91.7%  das imagens\n",
      "Portanto, o melhor dropout é o 0.1, que possui 92.0 de acurácia.\n"
     ]
    }
   ],
   "source": [
    "dropout_rates = [0.1, 0.3, 0.5, 0.7]\n",
    "\n",
    "melhor_acc = 0\n",
    "melhor_dropout = 0\n",
    "for dropout_rate in dropout_rates:\n",
    "    model = criar_modelo_com_dropout(melhor_layer, melhor_filter, melhor_dense, dropout_rate)\n",
    "    model.fit(train_images, train_labels, epochs=5, batch_size = 64)\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "    print(\"test_loss:\", test_loss, \"\\ntest_acc:\", test_acc)\n",
    "    print(f\"Essa acurácia significa que o modelo usando layer: {melhor_layer}, filter: {melhor_filter}, tamanho da camada densa: {melhor_dense} e dropout: {dropout_rate} é capaz de classificar corretamente {round(test_acc*100, 1)}%  das imagens\")\n",
    "    if test_acc > melhor_acc:\n",
    "        melhor_acc = test_acc\n",
    "        melhor_dropout = dropout_rate\n",
    "print(f\"Portanto, o melhor dropout é o {melhor_dropout}, que possui {round(melhor_acc*100, 1)} de acurácia.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E. O que é batch normalization? Aplique batch normalization e avalie como ela ajuda?\n",
    "\n",
    "Batch Normalization é uma técnica usada para acelerar o treinamento de redes neurais e estabilizar o processo de aprendizado. Ela normaliza a ativação de cada camada, aplicando uma transformação que mantém a média próxima de zero e o desvio padrão próximo de um. Isso ajuda a reduzir a covariância de ativação entre as camadas e torna o treinamento mais rápido e estável.\n",
    "\n",
    "Vamos modificar a função create_cnn_model_with_dense_dropout para adicionar uma camada Batch Normalization antes da camada densa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a arquitetura da CNN\n",
    "def criar_modelo_com_batchnorm(conv_layers, filters, dense_size, dropout_rate):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Adicionar camadas de convolução-pooling\n",
    "    for i in range(conv_layers):\n",
    "        model.add(layers.Conv2D(filters=filters, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu', input_shape = (28,28,1)))\n",
    "        print(\"Conv2D\")\n",
    "        if i != conv_layers - 1 or conv_layers == 1: #se não for a última camada de conv, faz MaxPooling2D, com exceção se tiver somente 1 camada de conv, nesse caso faz MaxPooling2D (mesmo ela sendo a última também)\n",
    "            print(\"MaxPooling2D\")\n",
    "            model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(dense_size, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(10, activation='softmax')) # 10 é o número de classes\n",
    "    \n",
    "    model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D\n",
      "MaxPooling2D\n",
      "Conv2D\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 99s 105ms/step - loss: 0.3500 - accuracy: 0.8754\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 105s 112ms/step - loss: 0.2348 - accuracy: 0.9149\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 100s 106ms/step - loss: 0.1958 - accuracy: 0.9281\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 96s 102ms/step - loss: 0.1668 - accuracy: 0.9392\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 96s 102ms/step - loss: 0.1438 - accuracy: 0.9471\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.2380 - accuracy: 0.9180\n",
      "test_loss: 0.2379808872938156 \n",
      "test_acc: 0.9179999828338623\n",
      "Essa acurácia significa que o modelo usando layer: 2, filter: 64, tamanho da camada densa: 256, dropout: 0.1 e com batch normalization é capaz de classificar corretamente 91.8%  das imagens\n",
      "Portanto, possui 91.8 de acurácia.\n"
     ]
    }
   ],
   "source": [
    "model = criar_modelo_com_batchnorm(melhor_layer, melhor_filter, melhor_dense, melhor_dropout)\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size = 64)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(\"test_loss:\", test_loss, \"\\ntest_acc:\", test_acc)\n",
    "print(f\"Essa acurácia significa que o modelo usando layer: {melhor_layer}, filter: {melhor_filter}, tamanho da camada densa: {melhor_dense}, dropout: {melhor_dropout} e com batch normalization é capaz de classificar corretamente {round(test_acc*100, 1)}%  das imagens\")\n",
    "\n",
    "print(f\"Portanto, possui {round(test_acc*100, 1)} de acurácia.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F. O que é data augmentation? Aplique data augmentation e avalie como ele ajuda?\n",
    "\n",
    "Data Augmentation é uma técnica usada para expandir o conjunto de dados de treinamento, aplicando transformações aleatórias nos dados existentes, como rotação, zoom, espelhamento, deslocamento, entre outros. Essa técnica é útil quando o conjunto de dados de treinamento é limitado, pois permite aumentar a diversidade dos exemplos apresentados ao modelo.\n",
    "\n",
    "Vamos usar a biblioteca imgaug para aplicar Data Augmentation no conjunto de dados Fashion MNIST. Primeiro, instale a biblioteca usando o seguinte comando: pip install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "\n",
    "# Definir as transformações de data augmentation\n",
    "augmenter = iaa.Sequential([\n",
    "    # iaa.Fliplr(0.5),  # Espelhamento horizontal com probabilidade de 0.5\n",
    "    iaa.Crop(percent=(0, 0.1)),  # Corte aleatório de até 10% da imagem\n",
    "    iaa.GaussianBlur(sigma=(0, 1.0)),  # Desfoque Gaussiano com desvio padrão entre 0 e 1.0\n",
    "    iaa.Affine(rotate=(-20, 20)),  # Rotação aleatória entre -20 e 20 graus\n",
    "    iaa.AddToHueAndSaturation(value=(-10, 10))  # Alteração aleatória no valor da saturação\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D\n",
      "MaxPooling2D\n",
      "Conv2D\n"
     ]
    }
   ],
   "source": [
    "model = criar_modelo_com_batchnorm(2, 64, 256, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Expected image with two or three dimensions, but got 4 dimensions and shape (23, 26, 1, 3).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\juju_\\OneDrive\\Documentos\\Sistemas-de-Informação\\5-Semestre\\IA\\EP1\\fashion_mnist.ipynb Cell 22\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juju_/OneDrive/Documentos/Sistemas-de-Informa%C3%A7%C3%A3o/5-Semestre/IA/EP1/fashion_mnist.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_images_uint8 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrepeat(train_images_uint8, \u001b[39m3\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juju_/OneDrive/Documentos/Sistemas-de-Informa%C3%A7%C3%A3o/5-Semestre/IA/EP1/fashion_mnist.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Aplicar Data Augmentation nos dados de treinamento\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/juju_/OneDrive/Documentos/Sistemas-de-Informa%C3%A7%C3%A3o/5-Semestre/IA/EP1/fashion_mnist.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_images_augmented \u001b[39m=\u001b[39m augmenter(images\u001b[39m=\u001b[39;49mtrain_images_uint8)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juju_/OneDrive/Documentos/Sistemas-de-Informa%C3%A7%C3%A3o/5-Semestre/IA/EP1/fashion_mnist.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Concatenar os dados originais com os dados aumentados\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juju_/OneDrive/Documentos/Sistemas-de-Informa%C3%A7%C3%A3o/5-Semestre/IA/EP1/fashion_mnist.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_images_combined \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([train_images_uint8, train_images_augmented])\n",
      "File \u001b[1;32mc:\\Users\\juju_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\imgaug\\augmenters\\meta.py:2008\u001b[0m, in \u001b[0;36mAugmenter.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2006\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   2007\u001b[0m     \u001b[39m\"\"\"Alias for :func:`~imgaug.augmenters.meta.Augmenter.augment`.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2008\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maugment(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\juju_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\imgaug\\augmenters\\meta.py:1979\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[1;34m(self, return_batch, hooks, **kwargs)\u001b[0m\n\u001b[0;32m   1968\u001b[0m \u001b[39m# augment batch\u001b[39;00m\n\u001b[0;32m   1969\u001b[0m batch \u001b[39m=\u001b[39m UnnormalizedBatch(\n\u001b[0;32m   1970\u001b[0m     images\u001b[39m=\u001b[39mimages,\n\u001b[0;32m   1971\u001b[0m     heatmaps\u001b[39m=\u001b[39mkwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mheatmaps\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1976\u001b[0m     line_strings\u001b[39m=\u001b[39mkwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mline_strings\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   1977\u001b[0m )\n\u001b[1;32m-> 1979\u001b[0m batch_aug \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maugment_batch_(batch, hooks\u001b[39m=\u001b[39;49mhooks)\n\u001b[0;32m   1981\u001b[0m \u001b[39m# return either batch or tuple of augmentables, depending on what\u001b[39;00m\n\u001b[0;32m   1982\u001b[0m \u001b[39m# was requested by user\u001b[39;00m\n\u001b[0;32m   1983\u001b[0m \u001b[39mif\u001b[39;00m return_batch:\n",
      "File \u001b[1;32mc:\\Users\\juju_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\imgaug\\augmenters\\meta.py:641\u001b[0m, in \u001b[0;36mAugmenter.augment_batch_\u001b[1;34m(self, batch, parents, hooks)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[39mwith\u001b[39;00m _maybe_deterministic_ctx(\u001b[39mself\u001b[39m):\n\u001b[0;32m    640\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batch_inaug\u001b[39m.\u001b[39mempty:\n\u001b[1;32m--> 641\u001b[0m         batch_inaug \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_augment_batch_(\n\u001b[0;32m    642\u001b[0m             batch_inaug,\n\u001b[0;32m    643\u001b[0m             random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[0;32m    644\u001b[0m             parents\u001b[39m=\u001b[39;49mparents \u001b[39mif\u001b[39;49;00m parents \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m [],\n\u001b[0;32m    645\u001b[0m             hooks\u001b[39m=\u001b[39;49mhooks)\n\u001b[0;32m    647\u001b[0m \u001b[39m# revert augmentables being set to None for non-activated augmenters\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m set_to_none:\n",
      "File \u001b[1;32mc:\\Users\\juju_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\imgaug\\augmenters\\meta.py:3124\u001b[0m, in \u001b[0;36mSequential._augment_batch_\u001b[1;34m(self, batch, random_state, parents, hooks)\u001b[0m\n\u001b[0;32m   3121\u001b[0m         order \u001b[39m=\u001b[39m sm\u001b[39m.\u001b[39mxrange(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m))\n\u001b[0;32m   3123\u001b[0m     \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m order:\n\u001b[1;32m-> 3124\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m[index]\u001b[39m.\u001b[39;49maugment_batch_(\n\u001b[0;32m   3125\u001b[0m             batch,\n\u001b[0;32m   3126\u001b[0m             parents\u001b[39m=\u001b[39;49mparents \u001b[39m+\u001b[39;49m [\u001b[39mself\u001b[39;49m],\n\u001b[0;32m   3127\u001b[0m             hooks\u001b[39m=\u001b[39;49mhooks\n\u001b[0;32m   3128\u001b[0m         )\n\u001b[0;32m   3129\u001b[0m \u001b[39mreturn\u001b[39;00m batch\n",
      "File \u001b[1;32mc:\\Users\\juju_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\imgaug\\augmenters\\meta.py:641\u001b[0m, in \u001b[0;36mAugmenter.augment_batch_\u001b[1;34m(self, batch, parents, hooks)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[39mwith\u001b[39;00m _maybe_deterministic_ctx(\u001b[39mself\u001b[39m):\n\u001b[0;32m    640\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batch_inaug\u001b[39m.\u001b[39mempty:\n\u001b[1;32m--> 641\u001b[0m         batch_inaug \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_augment_batch_(\n\u001b[0;32m    642\u001b[0m             batch_inaug,\n\u001b[0;32m    643\u001b[0m             random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[0;32m    644\u001b[0m             parents\u001b[39m=\u001b[39;49mparents \u001b[39mif\u001b[39;49;00m parents \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m [],\n\u001b[0;32m    645\u001b[0m             hooks\u001b[39m=\u001b[39;49mhooks)\n\u001b[0;32m    647\u001b[0m \u001b[39m# revert augmentables being set to None for non-activated augmenters\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m set_to_none:\n",
      "File \u001b[1;32mc:\\Users\\juju_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\imgaug\\augmenters\\size.py:1972\u001b[0m, in \u001b[0;36mCropAndPad._augment_batch_\u001b[1;34m(self, batch, random_state, parents, hooks)\u001b[0m\n\u001b[0;32m   1969\u001b[0m samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_draw_samples(random_state, shapes)\n\u001b[0;32m   1971\u001b[0m \u001b[39mif\u001b[39;00m batch\u001b[39m.\u001b[39mimages \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1972\u001b[0m     batch\u001b[39m.\u001b[39mimages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_augment_images_by_samples(batch\u001b[39m.\u001b[39;49mimages,\n\u001b[0;32m   1973\u001b[0m                                                    samples)\n\u001b[0;32m   1975\u001b[0m \u001b[39mif\u001b[39;00m batch\u001b[39m.\u001b[39mheatmaps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1976\u001b[0m     batch\u001b[39m.\u001b[39mheatmaps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_augment_maps_by_samples(\n\u001b[0;32m   1977\u001b[0m         batch\u001b[39m.\u001b[39mheatmaps,\n\u001b[0;32m   1978\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pad_mode_heatmaps, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pad_cval_heatmaps,\n\u001b[0;32m   1979\u001b[0m         samples)\n",
      "File \u001b[1;32mc:\\Users\\juju_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\imgaug\\augmenters\\size.py:2005\u001b[0m, in \u001b[0;36mCropAndPad._augment_images_by_samples\u001b[1;34m(self, images, samples)\u001b[0m\n\u001b[0;32m   2002\u001b[0m \u001b[39mfor\u001b[39;00m i, image \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(images):\n\u001b[0;32m   2003\u001b[0m     samples_i \u001b[39m=\u001b[39m samples[i]\n\u001b[1;32m-> 2005\u001b[0m     image_cr_pa \u001b[39m=\u001b[39m _crop_and_pad_arr(\n\u001b[0;32m   2006\u001b[0m         image, samples_i\u001b[39m.\u001b[39;49mcroppings, samples_i\u001b[39m.\u001b[39;49mpaddings,\n\u001b[0;32m   2007\u001b[0m         samples_i\u001b[39m.\u001b[39;49mpad_mode, samples_i\u001b[39m.\u001b[39;49mpad_cval, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkeep_size)\n\u001b[0;32m   2009\u001b[0m     result\u001b[39m.\u001b[39mappend(image_cr_pa)\n\u001b[0;32m   2011\u001b[0m \u001b[39mif\u001b[39;00m ia\u001b[39m.\u001b[39mis_np_array(images):\n",
      "File \u001b[1;32mc:\\Users\\juju_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\imgaug\\augmenters\\size.py:81\u001b[0m, in \u001b[0;36m_crop_and_pad_arr\u001b[1;34m(arr, croppings, paddings, pad_mode, pad_cval, keep_size)\u001b[0m\n\u001b[0;32m     77\u001b[0m height, width \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m:\u001b[39m2\u001b[39m]\n\u001b[0;32m     79\u001b[0m image_cr \u001b[39m=\u001b[39m _crop_arr_(arr, \u001b[39m*\u001b[39mcroppings)\n\u001b[1;32m---> 81\u001b[0m image_cr_pa \u001b[39m=\u001b[39m pad(\n\u001b[0;32m     82\u001b[0m     image_cr,\n\u001b[0;32m     83\u001b[0m     top\u001b[39m=\u001b[39;49mpaddings[\u001b[39m0\u001b[39;49m], right\u001b[39m=\u001b[39;49mpaddings[\u001b[39m1\u001b[39;49m],\n\u001b[0;32m     84\u001b[0m     bottom\u001b[39m=\u001b[39;49mpaddings[\u001b[39m2\u001b[39;49m], left\u001b[39m=\u001b[39;49mpaddings[\u001b[39m3\u001b[39;49m],\n\u001b[0;32m     85\u001b[0m     mode\u001b[39m=\u001b[39;49mpad_mode, cval\u001b[39m=\u001b[39;49mpad_cval)\n\u001b[0;32m     87\u001b[0m \u001b[39mif\u001b[39;00m keep_size:\n\u001b[0;32m     88\u001b[0m     image_cr_pa \u001b[39m=\u001b[39m ia\u001b[39m.\u001b[39mimresize_single_image(image_cr_pa, (height, width))\n",
      "File \u001b[1;32mc:\\Users\\juju_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\imgaug\\augmenters\\size.py:427\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(arr, top, right, bottom, left, mode, cval)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39m\"\"\"Pad an image-like array on its top/right/bottom/left side.\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \n\u001b[0;32m    358\u001b[0m \u001b[39mThis function is a wrapper around :func:`numpy.pad`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m \n\u001b[0;32m    424\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mimgaug\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdtypes\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39miadt\u001b[39;00m\n\u001b[1;32m--> 427\u001b[0m _assert_two_or_three_dims(arr)\n\u001b[0;32m    428\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m([v \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m [top, right, bottom, left]]), (\n\u001b[0;32m    429\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mExpected padding amounts that are >=0, but got \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    430\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m(top, right, bottom, left)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (top, right, bottom, left))\n\u001b[0;32m    432\u001b[0m is_multi_cval \u001b[39m=\u001b[39m ia\u001b[39m.\u001b[39mis_iterable(cval)\n",
      "File \u001b[1;32mc:\\Users\\juju_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\imgaug\\augmenters\\size.py:350\u001b[0m, in \u001b[0;36m_assert_two_or_three_dims\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(shape, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    349\u001b[0m     shape \u001b[39m=\u001b[39m shape\u001b[39m.\u001b[39mshape\n\u001b[1;32m--> 350\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(shape) \u001b[39min\u001b[39;00m [\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m], (\n\u001b[0;32m    351\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mExpected image with two or three dimensions, but got \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m dimensions \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    352\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mand shape \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mlen\u001b[39m(shape), shape))\n",
      "\u001b[1;31mAssertionError\u001b[0m: Expected image with two or three dimensions, but got 4 dimensions and shape (23, 26, 1, 3)."
     ]
    }
   ],
   "source": [
    "# Converter as imagens para uint8 e expandir a dimensão para três canais\n",
    "train_images_uint8 = np.expand_dims((train_images * 255).astype(np.uint8), axis=-1)\n",
    "train_images_uint8 = np.repeat(train_images_uint8, 3, axis=-1)\n",
    "\n",
    "# Aplicar Data Augmentation nos dados de treinamento\n",
    "train_images_augmented = augmenter(images=train_images_uint8)\n",
    "\n",
    "\n",
    "# Concatenar os dados originais com os dados aumentados\n",
    "train_images_combined = np.concatenate([train_images_uint8, train_images_augmented])\n",
    "train_labels_combined = np.concatenate([train_labels, train_labels])\n",
    "\n",
    "# Treinar o modelo com os dados aumentados\n",
    "model.fit(train_images_combined, train_labels_combined, epochs=5, batch_size=64, verbose=0)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(\"test_loss:\", test_loss, \"\\ntest_acc:\", test_acc)\n",
    "print(f\"Essa acurácia significa que o modelo usando layer: {melhor_layer}, filter: {melhor_filter}, tamanho da camada densa: {melhor_dense}, dropout: {dropout_rate}, com batch normalization e com data augmentation é capaz de classificar corretamente {round(test_acc*100, 1)}%  das imagens\")\n",
    "\n",
    "print(f\"Portanto, possui {round(test_acc*100, 1)} de acurácia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
